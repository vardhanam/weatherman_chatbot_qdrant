{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_api(lat, long):\n",
    "\n",
    "    # Define the base URL for the API\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "    # Define the parameters in a dictionary\n",
    "    params = {\n",
    "        'latitude': f'{lat}',\n",
    "        'longitude': f'{long}',\n",
    "        'forecast_days': '16',\n",
    "        'daily': ','.join([\n",
    "            'temperature_2m_max',\n",
    "            'temperature_2m_min',\n",
    "            'precipitation_sum',\n",
    "            'rain_sum',\n",
    "            'showers_sum',\n",
    "            'snowfall_sum',\n",
    "            'precipitation_hours',\n",
    "            'precipitation_probability_max',\n",
    "            'precipitation_probability_min',\n",
    "            'precipitation_probability_mean',\n",
    "            'sunrise',\n",
    "            'sunset',\n",
    "            'sunshine_duration',\n",
    "            'daylight_duration',\n",
    "            'wind_speed_10m_max',\n",
    "            'wind_gusts_10m_max',\n",
    "            'wind_direction_10m_dominant',\n",
    "            'shortwave_radiation_sum',\n",
    "            'et0_fao_evapotranspiration',\n",
    "            'uv_index_max',\n",
    "            'uv_index_clear_sky_max',\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # Make the GET request with the base URL and the parameters\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Process the response data\n",
    "        return data\n",
    "    else:\n",
    "        raise Exception(f\"Request failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_weather_data(weather_data):\n",
    "\n",
    "    # Extract daily weather data\n",
    "    daily_data = weather_data.get('daily', {})\n",
    "    time_data = daily_data.get('time', [])\n",
    "\n",
    "    # Prepare a list to hold the formatted strings for each time entry\n",
    "    formatted_entries = []\n",
    "\n",
    "    # Loop through each time entry and collect the corresponding weather features\n",
    "    for index, time_entry in enumerate(time_data):\n",
    "        features = [f\"Time: {time_entry}\"]\n",
    "        for feature, values in daily_data.items():\n",
    "            if feature != 'time':  # Skip the 'time' key since we already have it\n",
    "                value = values[index]\n",
    "                features.append(f\"{feature}: {value}\")\n",
    "        formatted_entries.append('\\n'.join(features))\n",
    "\n",
    "    # Join all entries separated by two newlines (for readability)\n",
    "    formatted_text = '\\n\\n'.join(formatted_entries)\n",
    "\n",
    "    # Print the formatted text\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def get_text_chunks_langchain(text):\n",
    "   text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "   docs = [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
    "   return docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latitude and longitude of Delhi are 28.6273928, 77.1716954\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "def get_lat_lon(location_name):\n",
    "    # Encode the location name to be URL-friendly\n",
    "    encoded_location_name = quote_plus(location_name)\n",
    "\n",
    "    # Use the OpenStreetMap Nominatim API for geocoding\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?format=json&q={encoded_location_name}\"\n",
    "\n",
    "    # Make an HTTP request to the service\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if any results were returned\n",
    "        if data:\n",
    "            # Extract the latitude and longitude from the first result\n",
    "            latitude = float(data[0]['lat'])\n",
    "            longitude = float(data[0]['lon'])\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            raise Exception(f\"No results found for location '{location_name}'\")\n",
    "    else:\n",
    "        raise Exception(\"Failed to connect to the geocoding service\")\n",
    "\n",
    "# Usage example:\n",
    "try:\n",
    "    location = \"Delhi\"\n",
    "    lat, lon = get_lat_lon(location)\n",
    "    print(f\"The latitude and longitude of {location} are {lat}, {lon}\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from typing import Optional\n",
    "\n",
    "import gradio as gr\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import (\n",
    "AutoTokenizer,\n",
    "AutoModelForCausalLM,\n",
    "BitsAndBytesConfig,\n",
    "pipeline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "\n",
    "    #Loading the Mistral Model\n",
    "    model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        )\n",
    "\n",
    "    # Building a LLM text-generation pipeline\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.1,\n",
    "        return_full_text=True,\n",
    "        max_new_tokens=1024,\n",
    "    )\n",
    "\n",
    "\n",
    "    return text_generation_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a756679b144976ab21bb9ec455d4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = load_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "def add_to_vectorstore(location):\n",
    "\n",
    "    lat, long = get_lat_lon(location)\n",
    "    data = weather_api(lat, long)\n",
    "    formatted_data = format_weather_data(data)\n",
    "    docs = get_text_chunks_langchain(formatted_data)\n",
    "\n",
    "    client = QdrantClient(\"localhost\", port=6333)\n",
    "    client.delete_collection(collection_name=\"my_weather_data\")\n",
    "\n",
    "    global qdrant\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        docs,\n",
    "        HuggingFaceEmbeddings(),\n",
    "        url=\"http://localhost:6333\",\n",
    "        prefer_grpc=True,\n",
    "        collection_name=\"my_weather_data\",\n",
    "    )\n",
    "\n",
    "    return f\"Weather data for latitude: {lat} & longitude: {long} added\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 563, which is longer than the specified 500\n",
      "Created a chunk of size 563, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n",
      "Created a chunk of size 565, which is longer than the specified 500\n",
      "Created a chunk of size 560, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n",
      "Created a chunk of size 562, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n",
      "Created a chunk of size 565, which is longer than the specified 500\n",
      "Created a chunk of size 561, which is longer than the specified 500\n",
      "Created a chunk of size 572, which is longer than the specified 500\n",
      "Created a chunk of size 574, which is longer than the specified 500\n",
      "Created a chunk of size 574, which is longer than the specified 500\n",
      "Created a chunk of size 570, which is longer than the specified 500\n",
      "Created a chunk of size 571, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "def answer_query(message, chat_history):\n",
    "    context_docs = qdrant.similarity_search(message, k= 16)\n",
    "    context = ' '.join(doc.page_content for doc in context_docs)\n",
    "\n",
    "    template = f\"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "\n",
    "        Question: {message}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    result = llm(template)\n",
    "\n",
    "\n",
    "    answer = result[0][\"generated_text\"].replace(template, '')\n",
    "\n",
    "\n",
    "    chat_history.append((message, answer))\n",
    "\n",
    "\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "\n",
    "    loc = gr.Textbox(label= \"Enter the location for weather data, for e.g Eiffel Tower, Delhi\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    loc.submit(add_to_vectorstore, loc, loc)\n",
    "    msg.submit(answer_query, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://a0f3ac923c4cf1ee4b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a0f3ac923c4cf1ee4b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 563, which is longer than the specified 500\n",
      "Created a chunk of size 563, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n",
      "Created a chunk of size 565, which is longer than the specified 500\n",
      "Created a chunk of size 560, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n",
      "Created a chunk of size 562, which is longer than the specified 500\n",
      "Created a chunk of size 564, which is longer than the specified 500\n",
      "Created a chunk of size 565, which is longer than the specified 500\n",
      "Created a chunk of size 561, which is longer than the specified 500\n",
      "Created a chunk of size 572, which is longer than the specified 500\n",
      "Created a chunk of size 574, which is longer than the specified 500\n",
      "Created a chunk of size 574, which is longer than the specified 500\n",
      "Created a chunk of size 570, which is longer than the specified 500\n",
      "Created a chunk of size 571, which is longer than the specified 500\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 693, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_4126/155692288.py\", line 13, in answer_query\n",
      "    result = llm(template)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 240, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    }
   ],
   "source": [
    "demo.queue()\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
